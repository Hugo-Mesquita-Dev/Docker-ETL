Pipeline de Dados com Docker, Spark e Airflow
https://img.shields.io/badge/license-MIT-blue.svg
https://img.shields.io/badge/Docker-%E2%89%A520.10.8-blue
https://img.shields.io/badge/Spark-3.5.0-red

Pipeline ETL completo para processamento de dados, com:
âœ… ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga (Spark)
âœ… OrquestraÃ§Ã£o (Airflow)
âœ… Armazenamento (PostgreSQL)
âœ… VisualizaÃ§Ã£o (Grafana + Metabase)
âœ… Monitoramento (Prometheus)

ğŸš€ ComeÃ§ando
PrÃ©-requisitos
Docker 20+

Docker Compose 2.10+

4GB+ RAM disponÃ­vel

bash
git clone https://github.com/seu-usuario/docker-etl-pipeline.git
cd docker-etl-pipeline
ğŸ› ï¸ ConfiguraÃ§Ã£o
Estrutura de Arquivos:

text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/          # Dados brutos (CSV/JSON)
â”‚   â””â”€â”€ output/         # Dados processados (Parquet)
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/           # Pipelines do Airflow
â”‚   â””â”€â”€ plugins/        # Plugins customizados
â”œâ”€â”€ grafana/
â”‚   â””â”€â”€ provisioning/   # Dashboards e fontes de dados
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etl_spark.py    # CÃ³digo principal do ETL
â”œâ”€â”€ docker-compose.yml  # DefiniÃ§Ã£o dos serviÃ§os
â””â”€â”€ Dockerfile          # Imagem customizada do Spark
Inicie os containers:

bash
docker-compose up -d --build
ğŸŒ Acesse as Ferramentas
ServiÃ§o	URL	Credenciais
Airflow	http://localhost:8080	admin/admin
Grafana	http://localhost:3001	admin/admin
Metabase	http://localhost:3000	Configurar 1Âº acesso
Prometheus	http://localhost:9090	-
ğŸ”„ Fluxo de Dados
Spark ETL processa dados de data/input/:

python
# scripts/etl_spark.py
df_vendas = spark.read.csv("/app/data/input/vendas.csv")
df_vendas.write.jdbc(url=DB_URL, table="vendas")
Airflow agenda execuÃ§Ãµes diÃ¡rias:

python
# airflow/dags/spark_etl_dag.py
with DAG('etl_daily', schedule_interval='@daily'):
    run_spark = DockerOperator(task_id='run_spark_etl')
Grafana visualiza os dados:

sql
SELECT categoria, SUM(valor) FROM vendas GROUP BY 1
ğŸ›‘ Comandos Ãšteis
FunÃ§Ã£o	Comando
Reiniciar serviÃ§os	docker-compose restart
Ver logs	docker-compose logs -f
Parar tudo	docker-compose down
Limpar dados	docker-compose down -v
Executar ETL manual	docker-compose run spark-etl
ğŸ“Š Dashboards Recomendados
Grafana:

Importe via ID:

10258 (Spark Monitoring)

9628 (PostgreSQL)

Metabase:

Crie questÃµes SQL com:

sql
SELECT * FROM vendas WHERE data > NOW() - INTERVAL '7 days'
â‰ï¸ SoluÃ§Ã£o de Problemas
Erro comum: "Tabela vendas nÃ£o existe"
bash
# Execute o ETL manualmente
docker-compose run spark-etl

# Verifique no PostgreSQL
docker-compose exec postgres psql -U postgres -d vendas_db -c "\dt"
Spark nÃ£o inicia
bash
docker-compose logs spark-etl | grep ERROR
ğŸ“„ LicenÃ§a
MIT License - Consulte o arquivo LICENSE para detalhes.

ContribuiÃ§Ãµes sÃ£o bem-vindas!
ğŸ”§ Reporte issues ou envie PRs.

âœ¨ Dica: ApÃ³s o primeiro setup, acesse http://localhost:8080 para configurar as DAGs no Airflow!